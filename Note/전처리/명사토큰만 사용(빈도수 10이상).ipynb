{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005s][warning][os,thread] Attempt to protect stack guard pages failed (0x00000001690ec000-0x00000001690f8000).\n",
      "[0.005s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "import nltk # national language tool kit\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data불러오기\n",
    "train = pd.read_csv(\"../../Data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "for i in range(0, 80):\n",
    "    train_list.append(train['가사'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 폰트 문제 해결 \n",
    "# matplotlib은 한글 폰트를 지원하지 않음\n",
    "# os정보\n",
    "import platform\n",
    "\n",
    "# font_manager : 폰트 관리 모듈\n",
    "# rc : 폰트 변경 모듈\n",
    "from matplotlib import font_manager, rc\n",
    "# unicode 설정\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic') # os가 macos\n",
    "elif platform.system() == 'Windows':\n",
    "    path = 'c:/Windows/Fonts/malgun.ttf' # os가 windows\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print(\"Unknown System\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 명사 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스를 한 줄씩 처리하기\n",
    "twitter = Okt()\n",
    "results4 = []\n",
    "lines = train_list\n",
    "for line in lines:\n",
    "    # 형태소 분석하기\n",
    "    train_malist = twitter.pos(line,norm=True,stem=True) # 단어의 기본형 사용\n",
    "    r = []\n",
    "    for word in train_malist:\n",
    "        # 어미, 조사, 구두점 등은 대상에서 제외\n",
    "        if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "            if word[1] in [\"Noun\"]:\n",
    "                if word[0] != \"n\":\n",
    "                    r.append(word[0]) # word[0] : data\n",
    "    rl4 = (\" \".join(r).strip())\n",
    "    \n",
    "    results4.append(rl4)\n",
    "    #print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list4 = str(results4).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사갯수 (중복포함)\n",
    "len(token_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 추가\n",
    "stop_words4 = ['내','너','네','우리','널','그대','넌','곳','나','수',\"속\",\"젠\",\"게\",\"알\",\"해\",\"거\",\"미\",\"일\",\"우린\",\"니\",\"젠\",\"때\",\"알\",\"걸\"\\\n",
    "                \"두\",\"손\",\"눈\",\"것\",\"이\",\"두\",\"볼\",\"어디\",\"하나요\",\"아무\",\"길\",\"좀\",\"뒤\",\"척\",\"안\",\"마\",\"모두\",\"여기\",\"건\",\"비\",\"품\",\"번\",\"줄\"\\\n",
    "                ,\"난난\",\"살\",\"그\",\"더\",\"난\",\"사람\",\"걸\",\"'너\",\"나나\",\"땐\",\"적\",\"저기\",\"채\",\"입\",\"그것\",\"위\",\"해도\",\"수가\",\"둘\",\\\n",
    "                \"듯\",\"수',\",\"사랑',\",\"롤\",\"요\",\"저\",\"감\",\"'사랑\",\"다시',\",\"라면\",\"은\",\"뭐\",\"땜\",\"워\",\"준\",\"불\",\"뿐\",\"서서\",\"순\",\"울\",\\\n",
    "                \"우릴\",\"남\",\"발\",\"춤\",\"일도\",\"란\",\"전\",\"온\",\"중\",\"말',\",\"대도\",\"만\",\"낼\",\"빈\",\"오지\",\"덜\",\"방\",\"무\",\"진\",\"투\",\"드\",\"돌\",\\\n",
    "                \"바랬는데\",\"빗\",\"오\",\"보\",\"거지\",\"후\",\"핀\",\"잡\",\"부시\",\"거더\",\"알',\",\"게',\",\"'혼자\",\"수도\",\"우리둘\",\"래야\",\"통해\",\"'하루\",\\\n",
    "                \"애\",\"아들\",\"더욱더\",\"전하\",\"무엇\",\"끝내기\",\"티\",\"겉\",\"헤어지자\",\"사이',\",\"'나\",\"'내\",\"더',\",\"굽\",\"녹\",\"테\",\"세\",\"'오늘\",\\\n",
    "                \"것',\",\"'그대\",\"그대',\",\"너',\",\"날',\",\"친구\",\"내겐\",\"단\",\"다가\",\"누가\",\"듯이\",\"안고\",\"물\",\"활\",\"첫\",\"목\",\"픈\",\"바\",\"몇\",\\\n",
    "                \"쉬\",\"철\",\"나라\",\"죽\",\"찬\",\"지고\",\"나로\",\"감고\",\"보이\",\"운\",\"다한\",\"룰\",\"식이\",\"이면\",\"날수\",\"남지\",\"만이\",\"째깍\",\"번이\",\"날\"\n",
    "                ]\n",
    "token_ko4 = [each_word for each_word in token_list4 if each_word not in stop_words4]\n",
    "ko4 = nltk.Text(token_ko4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 명사의 경우 명사의 갯수가 1개인 경우가 총 402번이였으며 특수기호 포함이 가장 많이 들어있음을 확인하고 1번 사용된 명사는 토큰에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = []\n",
    "for key, val in zip(list(ko4.vocab().keys()),list(ko4.vocab().values())):\n",
    "    if val < 10:\n",
    "        testing.append([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing2 = []\n",
    "for i in range(len(testing)):\n",
    "    testing2.append(str(testing[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번 사용된 언어 지운후 다시 확인\n",
    "token_ko5 = set(ko4.vocab()) - set(testing2)\n",
    "token_ko6 = [each_word for each_word in token_list4 if each_word in token_ko5]\n",
    "ko6 = nltk.Text(token_ko6)\n",
    "ko6.vocab().most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 토큰 갯수 (중복제외)\n",
    "len(list(ko6.vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 50개의 빈도수 차트 만들기\n",
    "plt.figure(figsize=(12,6))\n",
    "ko6.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 100개만 wordcloud로 그리기\n",
    "# 구한데이터로 그리기\n",
    "data4 = ko6.vocab().most_common(100)\n",
    "\n",
    "# 딕셔너리로 만들기\n",
    "tmp_data4 = dict(data4)\n",
    "\n",
    "wordcloud4 = WordCloud(\n",
    "    background_color='White',\n",
    "    relative_scaling=0.8,\n",
    "    font_path=\"AppleGothic\" \n",
    ").generate_from_frequencies(tmp_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud4, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 추출한 토큰 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 토큰\n",
    "print(\"명사토큰 : \\n\",list(ko6.vocab()),\" =>명사 토큰 갯수 : \",len(list(ko6.vocab())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sum = list(ko6.vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 토큰 one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가사 데이터 okt로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스를 한 줄씩 처리하기\n",
    "twitter = Okt() \n",
    "okt_result = []\n",
    "lines = train['가사']\n",
    "for line in lines:\n",
    "    # 형태소 분석하기\n",
    "    train_malist = twitter.pos(line,norm=True,stem=True) # 단어의 기본형 사용\n",
    "    r = []\n",
    "    for word in train_malist:\n",
    "        if word[1] in [\"Noun\"]:\n",
    "            if word[0] != \"n\":\n",
    "                r.append(word[0])\n",
    "    rl3 = (\" \".join(r).strip())\n",
    "    \n",
    "    okt_result.append(rl3)\n",
    "    # print(rl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동사와 명사로 분리가 잘 되었는지 확인하기\n",
    "okt_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 0은 없는 단어와 padding으로 집어 넣기 나머지 단어들은 1부터 순차적으로 인덱스 부여\n",
    "\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(token_sum)}\n",
    "word_to_index['pad'] = 0 # 패딩용 인덱스 0\n",
    "word_to_index['unk'] = 0 # unknown 용 인덱스 0\n",
    "\n",
    "\n",
    "# 기존 훈련 데이터에서 각 단어를 고유한 정수로 부여\n",
    "encoded = []\n",
    "for line in okt_result: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      try:\n",
    "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((encoded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 전체 길이 padding작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정해줄 길이를 찾기 위해 가장 긴 리뷰 길이 확인\n",
    "max_length = max(len(l) for l in encoded)\n",
    "print('가사의 최대 길이 : %d' % max_length)\n",
    "print('가사의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('가사의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
    "plt.hist([len(s) for s in encoded], bins=50)\n",
    "plt.xlabel('length of sample')\n",
    "plt.ylabel('number of sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장마다 단어 갯수가 다르므로 sequence padding을 넣어서 문장의 길이 동일하게 만들기 \n",
    "# 이거하려고 위에서 단어갯수 체크한거임\n",
    "\n",
    "padding_paragraphs_encoding = keras.preprocessing.sequence.pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "padding_paragraphs_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(padding_paragraphs_encoding)), len(list(padding_paragraphs_encoding)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류항목['happy', 'sad', 'regret', 'indifference']를 수치로 변경하기\n",
    "categories = train['감정'].to_list()\n",
    "\n",
    "def category_encoding(category):\n",
    "    if category == 'happy':\n",
    "        return 0\n",
    "    elif category == 'sad':\n",
    "        return 1\n",
    "    elif category == 'regret':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_category = [category_encoding(category) for category in categories]\n",
    "encoded_category[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 가사 감정 RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test data 만들기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    padding_paragraphs_encoding, encoded_category, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val data 만들기\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    train_input, train_target, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow import keras \n",
    "\n",
    "# GPU 사용 설정\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input.shape, val_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# 문맥 생성 단계\n",
    "model.add(keras.layers.SimpleRNN(64, input_shape=(558, 71),dropout=0.1))\n",
    "model.add(keras.layers.Dense(4, activation='softmax')) # 문장은 확률값을 구해야하므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = keras.utils.to_categorical(train_input)\n",
    "train_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(train_oh[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_oh = keras.utils.to_categorical(val_input)\n",
    "val_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "model.compile(\n",
    "    optimizer=rmsprop,\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = 'accuracy'\n",
    "    )\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Data/best-train_rnn-model.h5\")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    restore_best_weights= True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    np.array(train_oh),\n",
    "    np.array(train_target),\n",
    "    epochs=1000,\n",
    "    #batch_size=64, # mini batch 경사 하강법\n",
    "    validation_data = (np.array(val_oh), np.array(val_target)),\n",
    "    # callbacks = [checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 해보기\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax1 = fig.add_subplot(1,2,1) # 1행 2열의 첫번째다.\n",
    "ax2 = fig.add_subplot(1,2,2) # 1행 2열의 두번째다.\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax2.set_xlabel('epoch')\n",
    "\n",
    "ax1.set_ylabel('loss')\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "ax1.legend(['train','val'])\n",
    "ax2.legend(['train','val'])\n",
    "\n",
    "\n",
    "ax1.set_title(\"RNN - loss\", size=20)\n",
    "ax2.set_title(\"RNN - accuracy\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c8100a7997e958d8100731b108691612e131425235a19f9419f6b3b0ff44b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
